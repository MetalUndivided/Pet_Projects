{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3acece08",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Imports\" data-toc-modified-id=\"Data-Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Imports</a></span></li><li><span><a href=\"#Data-Examination\" data-toc-modified-id=\"Data-Examination-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Examination</a></span><ul class=\"toc-item\"><li><span><a href=\"#Photos\" data-toc-modified-id=\"Photos-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Photos</a></span></li><li><span><a href=\"#Label-distribution\" data-toc-modified-id=\"Label-distribution-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Label distribution</a></span></li></ul></li><li><span><a href=\"#Network-implementation\" data-toc-modified-id=\"Network-implementation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Network implementation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bd41ad",
   "metadata": {},
   "source": [
    "# Bird Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f740b2",
   "metadata": {},
   "source": [
    "Built using the Kaggle sourced <a href=https://www.kaggle.com/datasets/gpiosenka/100-bird-species>dataset</a> of 525 bird species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e2d576",
   "metadata": {},
   "source": [
    "## Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d595f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pickle\n",
    "from contextlib import nullcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e494287",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43a892d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb8969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/media/metal_undivided/DATA/Datasets/Kaggle_525_species/'\n",
    "\n",
    "train_dataset = datasets.ImageFolder(data_dir + 'train/', transform=transform)\n",
    "val_dataset = datasets.ImageFolder(data_dir + 'valid/', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(data_dir + 'test/', transform=transform)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc4be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next(iter(train_dataloader))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bdc704",
   "metadata": {},
   "source": [
    "The data is successfully imported, let's examine it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e8f3d",
   "metadata": {},
   "source": [
    "## Data Examination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcbd69a",
   "metadata": {},
   "source": [
    "### Photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bff406",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels, labels = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a460ebd8",
   "metadata": {},
   "source": [
    "Let's also get the label-species pairs for visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31877ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = train_dataset.find_classes(data_dir + 'train/')\n",
    "species = {label:name for name, label in species[1].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7567ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 4, figsize=(20, 20))\n",
    "\n",
    "for i in range(16):\n",
    "    axobject = axs[i // 4][i % 4]\n",
    "    \n",
    "    axobject.imshow(pixels[i].permute(1, 2, 0))\n",
    "    axobject.set_title(species[labels.numpy()[i]])\n",
    "    axobject.get_xaxis().set_visible(False)\n",
    "    axobject.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb9557e",
   "metadata": {},
   "source": [
    "Looks good, both the names and the images.\n",
    "\n",
    "Before implementing the network, let's see how many photos of each bird we have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1adf4",
   "metadata": {},
   "source": [
    "### Label distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb74710",
   "metadata": {},
   "source": [
    "Since iterating over all the classes is fairly time consuming, this section is normally omitted and counts precalculated here are used instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b734d4",
   "metadata": {},
   "source": [
    "```python\n",
    "species_count = torch.zeros(525)\n",
    "species_count.requires_grad = False\n",
    "\n",
    "for counter, batch in enumerate(tqdm(train_dataloader)):\n",
    "    _, labels = batch\n",
    "    species_count += nn.functional.one_hot(labels, num_classes=525).sum(axis=0)    \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2273ca4d",
   "metadata": {},
   "source": [
    "```python\n",
    "torch.save(species_count, 'species_count')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e507a",
   "metadata": {},
   "source": [
    "Loading precalculated counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc8ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_count = torch.load('species_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce1c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(species_count.numpy(), bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1a9f2",
   "metadata": {},
   "source": [
    "Most of the classes are fairly equally represented, save for a few outliers.\n",
    "\n",
    "In order to improve learning, we'll use the inverse of these counts as class weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6be9b9",
   "metadata": {},
   "source": [
    "## Network implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a285f8",
   "metadata": {},
   "source": [
    "For this task I'll be implementing a ResNet network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9771d431-1931-43be-81c3-4ba5333b404e",
   "metadata": {},
   "source": [
    "Since ResNet bottleneck blocks are identical for all of ResNet50+ family, I'll first define them as their own class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5ea310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_labels, freeze_layers=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.freeze_layers = freeze_layers\n",
    "        \n",
    "        self.base = nn.Conv2d(3, 64, (7, 7), stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(3, 3), stride=2, padding=1)\n",
    "        \n",
    "        self.block1 = self.ResNetLayer(BottleneckBlock, 64, 64, 256, 3)\n",
    "        self.block2 = self.ResNetLayer(BottleneckBlock, 256, 128, 512, 4)\n",
    "        self.block3 = self.ResNetLayer(BottleneckBlock, 512, 256, 1024, 6)\n",
    "        self.block4 = self.ResNetLayer(BottleneckBlock, 1024, 512, 2048, 3)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.linear = nn.Linear(2048, n_labels, bias=True)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        with torch.no_grad() if self.freeze_layers else null_context():\n",
    "        \n",
    "            x = self.base(x)\n",
    "            x = self.bn1(x)\n",
    "            x = nn.ReLU()(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "            x = self.block1(x)\n",
    "            x = self.block2(x)\n",
    "            #print(x)\n",
    "            x = self.block3(x)\n",
    "            x = self.block4(x)\n",
    "\n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            \n",
    "        x = self.linear(x)\n",
    "        #x = nn.Softmax(dim=1)(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def ResNetLayer(self, block, input_channels, interm_channels, output_channels, repetitions, first_layer=False):\n",
    "        \n",
    "        layer = nn.Sequential()\n",
    "        \n",
    "        layer.append(block(input_channels, interm_channels, output_channels, stride=2, residual_downsample=True))\n",
    "        \n",
    "        for repetition in range(repetitions - 1):\n",
    "            layer.append(block(output_channels, interm_channels,output_channels))\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74614a78-a2cc-4f6e-82fd-6ad9068e857d",
   "metadata": {},
   "source": [
    "Now that the main architecture is described, I need to implement the main building block for the model - the bottleneck block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e6dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels, interm_channels, output_channels, stride=1, residual_downsample=False):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.interm_channels = interm_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.residual_downsample = residual_downsample\n",
    "        \n",
    "        self.layer1 = nn.Conv2d(input_channels, interm_channels, (1, 1), stride=stride)\n",
    "        self.bn1 = nn.BatchNorm2d(interm_channels)\n",
    "        self.layer2 = nn.Conv2d(interm_channels, interm_channels, (3, 3), stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(interm_channels)\n",
    "        self.layer3 = nn.Conv2d(interm_channels, output_channels, (1, 1), stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(output_channels)\n",
    "        if residual_downsample:\n",
    "            self.residual = nn.Conv2d(input_channels, output_channels, (1, 1), stride=stride)\n",
    "            self.res_bn = nn.BatchNorm2d(output_channels)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_init = x\n",
    "        x = self.layer1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        if self.residual_downsample:\n",
    "            x_init = self.residual(x_init)\n",
    "            x_init = self.res_bn(x_init)\n",
    "            x += x_init\n",
    "        else:\n",
    "            x+= x_init\n",
    "            \n",
    "        x = nn.ReLU()(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c01791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_batch(model, features, target, optimizer, loss_fn):\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    predictions = model(features)\n",
    "    \n",
    "    loss = loss_fn(predictions, target)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a80718-b40a-4e72-b7e3-3389be02c541",
   "metadata": {},
   "source": [
    "Since the dataset is fairly light (around 200 images per class), it's unlikely that the model will learn to generalize well enough, so I'll be performing transfer learning and finetuning the model for our specific dataset.\n",
    "\n",
    "This time I'll be using IMAGENET weights (1000 classes, ~1.2kk training images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8f6e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = models.resnet50(weights=\"IMAGENET1K_V2\").state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a6fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ResNet(n_labels=525, freeze_layers=True)\n",
    "model = models.resnet50(weights=\"IMAGENET1K_V2\")\n",
    "\n",
    "#model.load_state_dict(pretrained_weights, strict=False)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()#weight=(species_count.mean() / species_count) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda6c801-8689-47bf-af6b-8e0178eb3b88",
   "metadata": {},
   "source": [
    "```\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6268574-bf41-48a6-930f-09bfdf42a1f4",
   "metadata": {},
   "source": [
    "```\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 525)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a243853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(True)\n",
    "\n",
    "for epoch in range(5):\n",
    "\n",
    "    ## training\n",
    "    total_loss = 0\n",
    "    for counter, batch in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "        features, target = batch\n",
    "\n",
    "        total_loss += fit_batch(model, features, target, optimizer, loss_fn).item() \n",
    "\n",
    "        if counter % 100 == 99:\n",
    "            last_loss = total_loss / len(batch) / 100\n",
    "            print('Batch {1} loss: {0:.8f}'.format(last_loss, counter + 1))\n",
    "            total_loss = 0\n",
    "            \n",
    "    ## validation\n",
    "    with torch.no_grad():\n",
    "        correct_labels = 0\n",
    "        \n",
    "        for counter, batch in enumerate(val_dataloader):\n",
    "            \n",
    "            pixels, labels = batch\n",
    "            \n",
    "            ## all predictions\n",
    "            predictions = torch.max(model(pixels), dim=1)[1]\n",
    "            ## number of correct predictions\n",
    "            predictions = torch.sum(predictions == labels[0])\n",
    "            correct_labels += predictions\n",
    "        \n",
    "        print('Epoch {0} validation accuracy {1:.6%}'.format(epoch + 1, correct_labels / (counter * batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda3ebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model\", \"wb\") as model_file:\n",
    "    pickle.dump(model, model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "270px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
